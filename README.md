# Q&A Приложение с использованием LLM

Это приложение использует большую языковую модель (LLM) для ответа на вопросы, основываясь на контексте.

## Настройка окружения

1.  **Установите Ollama:** Следуйте инструкциям на [официальном сайте Ollama](https://ollama.com/).
2.  **Загрузите модель:** Выполните следующую команду в терминале, чтобы загрузить необходимую модель qwen2.5:1.5b-instruct-q8_0:
    ```bash
    ollama run qwen2.5:1.5b-instruct-q8_0
    ```
3.  **Создайте файл `.env`:** В корневой директории проекта создайте файл с именем `.env` и добавьте в него следующие строки:
    ```dotenv
    API_KEY=OLLAMA
    BASE_URL=http://localhost:11434/v1
    MODEL=qwen2.5:1.5b-instruct-q8_0
    ```
    *   `API_KEY`: Установите значение `OLLAMA`, если вы используете Ollama локально.
    *   `BASE_URL`: URL вашего локального сервера Ollama. Обычно это `http://localhost:11434/v1`.
    *   `MODEL`: Точное имя модели, которое вы загрузили с помощью команды `ollama run`.

## Запуск приложения

Используйте `uv` для запуска Streamlit приложения:
```bash
uv run streamlit run app.py
```
Приложение откроется в вашем веб-браузере.

## Тестирование

Testing скрипт показал 90% точности на выборке из 100 случайных экземпляров данных.
