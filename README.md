# Q&A Приложение с использованием LLM

Это приложение использует большую языковую модель (LLM) для ответа на вопросы, основываясь на контексте.

## Настройка окружения

1.  **Установите Ollama:** Следуйте инструкциям на [официальном сайте Ollama](https://ollama.com/).
2.  **Загрузите модель:** Выполните следующую команду в терминале, чтобы загрузить необходимую модель Qwen2.5-1.5B:
    ```bash
    ollama run hf.co/Qwen/Qwen2.5-1.5B-Instruct-GGUF:Q4_K_M
    ```
3.  **Создайте файл `.env`:** В корневой директории проекта создайте файл с именем `.env` и добавьте в него следующие строки:
    ```dotenv
    API_KEY=OLLAMA
    BASE_URL=http://localhost:11434/v1
    MODEL=hf.co/Qwen/Qwen2.5-1.5B-Instruct-GGUF:Q4_K_M
    ```
    *   `API_KEY`: Установите значение `OLLAMA`, если вы используете Ollama локально.
    *   `BASE_URL`: URL вашего локального сервера Ollama. Обычно это `http://localhost:11434/v1`.
    *   `MODEL`: Точное имя модели, которое вы загрузили с помощью команды `ollama run`.

## Запуск приложения
    Используйте `uv` для запуска Streamlit приложения:
    ```bash
    uv run streamlit run app.py
    ```
    Приложение откроется в вашем веб-браузере.