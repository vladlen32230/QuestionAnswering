# Q&A Приложение с использованием LLM

Это приложение использует большие языковые модели (LLM) для ответа на вопросы, основываясь на контексте. Оно позволяет выбирать между двумя различными моделями через интерфейс Streamlit.

## Настройка окружения

1.  **Установите Ollama:** Следуйте инструкциям на [официальном сайте Ollama](https://ollama.com/).
2.  **Загрузите модели:** Выполните следующие команды в терминале, чтобы загрузить необходимые модели:
    ```bash
    ollama run qwen3:0.6b-q8_0
    ollama run qwen3:4b-q8_0
    ```
3.  **Создайте файл `.env`:** В корневой директории проекта создайте файл с именем `.env` и добавьте в него следующие строки:
    ```dotenv
    API_KEY=OLLAMA
    BASE_URL=http://localhost:11434/v1
    ```
    *   `API_KEY`: Установите значение `OLLAMA`, если вы используете Ollama локально.
    *   `BASE_URL`: URL вашего локального сервера Ollama. Обычно это `http://localhost:11434/v1`.

## Запуск приложения

Используйте `uv` для запуска Streamlit приложения:
```bash
uv run streamlit run app.py
```
Приложение откроется в вашем веб-браузере. Вы сможете выбрать одну из доступных моделей (`qwen3:0.6b-q8_0` или `qwen3:4b-q8_0`) перед тем, как задать вопрос.

## Тестирование

Testing скрипт показал 81% точности на выборке из 100 случайных экземпляров данных для qwen3-0.6b
98% точности для qwen3-4b
